{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Number of Classes: 10\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1179776   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 1,199,882\n",
      "Trainable params: 1,199,882\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import os\n",
    "\n",
    "# Training Parameters\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "# loads the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test)  = mnist.load_data()\n",
    "\n",
    "# Lets store the number of rows and columns\n",
    "img_rows = x_train[0].shape[0]\n",
    "img_cols = x_train[1].shape[0]\n",
    "\n",
    "# Getting our date in the right 'shape' needed for Keras\n",
    "# We need to add a 4th dimenion to our date thereby changing our\n",
    "# Our original image shape of (60000,28,28) to (60000,28,28,1)\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "\n",
    "# store the shape of a single image \n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "# change our image type to float32 data type\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Normalize our data by changing the range from (0 to 255) to (0 to 1)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# Now we one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# Let's count the number columns in our hot encoded matrix \n",
    "print (\"Number of Classes: \" + str(y_test.shape[1]))\n",
    "\n",
    "num_classes = y_test.shape[1]\n",
    "num_pixels = x_train.shape[1] * x_train.shape[2]\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = keras.optimizers.Adadelta(),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "                     \n",
    "checkpoint = ModelCheckpoint(\"MNIST_Checkpoint.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "callbacks = [checkpoint]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: 0.2171 - accuracy: 0.9335 - val_loss: 0.0507 - val_accuracy: 0.9823\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05068, saving model to MNIST_Checkpoint.h5\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 8s 140us/step - loss: 0.0820 - accuracy: 0.9757 - val_loss: 0.0398 - val_accuracy: 0.9865\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05068 to 0.03976, saving model to MNIST_Checkpoint.h5\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 9s 143us/step - loss: 0.0631 - accuracy: 0.9813 - val_loss: 0.0344 - val_accuracy: 0.9884\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03976 to 0.03444, saving model to MNIST_Checkpoint.h5\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.0529 - accuracy: 0.9841 - val_loss: 0.0332 - val_accuracy: 0.9891\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03444 to 0.03316, saving model to MNIST_Checkpoint.h5\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0485 - accuracy: 0.9858 - val_loss: 0.0301 - val_accuracy: 0.9898\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03316 to 0.03008, saving model to MNIST_Checkpoint.h5\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 9s 146us/step - loss: 0.0438 - accuracy: 0.9870 - val_loss: 0.0329 - val_accuracy: 0.9892\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.03008\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 9s 146us/step - loss: 0.0418 - accuracy: 0.9873 - val_loss: 0.0321 - val_accuracy: 0.9901\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.03008\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 9s 147us/step - loss: 0.0385 - accuracy: 0.9881 - val_loss: 0.0288 - val_accuracy: 0.9894\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.03008 to 0.02881, saving model to MNIST_Checkpoint.h5\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 9s 151us/step - loss: 0.0377 - accuracy: 0.9885 - val_loss: 0.0341 - val_accuracy: 0.9903\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.02881\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 9s 155us/step - loss: 0.0353 - accuracy: 0.9895 - val_loss: 0.0318 - val_accuracy: 0.9909\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.02881\n",
      "Test loss: 0.03177153588802648\n",
      "Test accuracy: 0.9908999800682068\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "          batch_size = batch_size,\n",
    "          epochs = epochs,\n",
    "          verbose = 1,\n",
    "          callbacks = callbacks,\n",
    "          validation_data = (x_test, y_test))\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Multiple Call Backs & Early Stopping\n",
    "\n",
    "We can use other call back methods to monitor our training process such as **Early Stopping**. Checkout the Keras documentation for more:\n",
    "- https://keras.io/callbacks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', # value being monitored for improvement\n",
    "                          min_delta = 0, #Abs value and is the min change required before we stop\n",
    "                          patience = 3, #Number of epochs we wait before stopping \n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True) #keeps the best weigths once stopped\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [earlystop, checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can attempt to run again to see if it worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 10s 162us/step - loss: 0.0281 - accuracy: 0.9915 - val_loss: 0.0297 - val_accuracy: 0.9913\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.02514\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 0.0268 - accuracy: 0.9922 - val_loss: 0.0300 - val_accuracy: 0.9925\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.02514\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 10s 160us/step - loss: 0.0264 - accuracy: 0.9922 - val_loss: 0.0257 - val_accuracy: 0.9926\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.02514\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 9s 158us/step - loss: 0.0264 - accuracy: 0.9922 - val_loss: 0.0267 - val_accuracy: 0.9922\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.02514\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 9s 149us/step - loss: 0.0257 - accuracy: 0.9919 - val_loss: 0.0254 - val_accuracy: 0.9922\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.02514\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 9s 153us/step - loss: 0.0258 - accuracy: 0.9924 - val_loss: 0.0283 - val_accuracy: 0.9916\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.02514\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.0233 - accuracy: 0.9929 - val_loss: 0.0303 - val_accuracy: 0.9919\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.02514\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 9s 152us/step - loss: 0.0229 - accuracy: 0.9930 - val_loss: 0.0251 - val_accuracy: 0.9931\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02514 to 0.02509, saving model to MNIST_Checkpoint.h5\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 0.0236 - accuracy: 0.9932 - val_loss: 0.0267 - val_accuracy: 0.9931\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.02509\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 9s 154us/step - loss: 0.0233 - accuracy: 0.9929 - val_loss: 0.0321 - val_accuracy: 0.9905\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.02509\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 9s 156us/step - loss: 0.0221 - accuracy: 0.9934 - val_loss: 0.0296 - val_accuracy: 0.9920\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.02509\n",
      "Epoch 00011: early stopping\n",
      "Test loss: 0.025087458808830523\n",
      "Test accuracy: 0.9930999875068665\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=30,\n",
    "          verbose=1,\n",
    "          callbacks = callbacks,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another useful callback is Reducing our learning Rate on Plateau\n",
    "\n",
    "We can avoid having our oscillate around the global minimum by attempting to reduce the Learn Rate by a certain fact. If no improvement is seen in our monitored metric (val_loss typically), we wait a certain number of epochs (patience) then this callback reduces the learning rate by a factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 3, verbose = 1, min_delta = 0.0001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
